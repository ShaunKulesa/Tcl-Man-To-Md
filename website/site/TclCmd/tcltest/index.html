<!DOCTYPE html>
<html class="writer-html5" lang="en" >
<head>
    <meta charset="utf-8" />
    <meta http-equiv="X-UA-Compatible" content="IE=edge" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" /><link rel="canonical" href="https://www.tcl.tk/man/TclCmd/tcltest/" />
      <link rel="shortcut icon" href="../../img/favicon.ico" />
    <title>NAME - Tcl/Tk</title>
    <link rel="stylesheet" href="../../css/theme.css" />
    <link rel="stylesheet" href="../../css/theme_extra.css" />
    
      <script>
        // Current page data
        var mkdocs_page_name = "NAME";
        var mkdocs_page_input_path = "TclCmd\\tcltest.md";
        var mkdocs_page_url = "/man/TclCmd/tcltest/";
      </script>
    
    <script src="../../js/jquery-3.6.0.min.js" defer></script>
    <!--[if lt IE 9]>
      <script src="../../js/html5shiv.min.js"></script>
    <![endif]--> 
</head>

<body class="wy-body-for-nav" role="document">

  <div class="wy-grid-for-nav">
    <nav data-toggle="wy-nav-shift" class="wy-nav-side stickynav">
    <div class="wy-side-scroll">
      <div class="wy-side-nav-search">
          <a href="../.." class="icon icon-home"> Tcl/Tk
        </a><div role="search">
  <form id ="rtd-search-form" class="wy-form" action="../../search.html" method="get">
      <input type="text" name="q" placeholder="Search docs" aria-label="Search docs" title="Type search term here" />
  </form>
</div>
      </div>

      <div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="Navigation menu">
              <p class="caption"><span class="caption-text">Home</span></p>
              <ul>
                  <li class="toctree-l1"><a class="reference internal" href="../..">Tcl/Tk</a>
                  </li>
                  <li class="toctree-l1"><a class="reference internal" href="#">ItclCmd</a>
    <ul>
                <li class="toctree-l2"><a class="reference internal" href="../../ItclCmd/">ItclCmd</a>
                </li>
    </ul>
                  </li>
                  <li class="toctree-l1"><a class="reference internal" href="#">ItclLib</a>
    <ul>
                <li class="toctree-l2"><a class="reference internal" href="../../ItclLib/">ItclLib</a>
                </li>
    </ul>
                  </li>
                  <li class="toctree-l1"><a class="reference internal" href="#">Keywords</a>
    <ul>
                <li class="toctree-l2"><a class="reference internal" href="../../Keywords/">Keywords</a>
                </li>
    </ul>
                  </li>
                  <li class="toctree-l1"><a class="reference internal" href="#">SqliteCmd</a>
    <ul>
                <li class="toctree-l2"><a class="reference internal" href="../../SqliteCmd/">SqliteCmd</a>
                </li>
    </ul>
                  </li>
                  <li class="toctree-l1"><a class="reference internal" href="#">TclCmd</a>
    <ul>
                <li class="toctree-l2"><a class="reference internal" href="../">TclCmd</a>
                </li>
    </ul>
                  </li>
                  <li class="toctree-l1"><a class="reference internal" href="#">TclLib</a>
    <ul>
                <li class="toctree-l2"><a class="reference internal" href="../../TclLib/">TclLib</a>
                </li>
    </ul>
                  </li>
                  <li class="toctree-l1"><a class="reference internal" href="#">TdbcCmd</a>
    <ul>
                <li class="toctree-l2"><a class="reference internal" href="../../TdbcCmd/">TdbcCmd</a>
                </li>
    </ul>
                  </li>
                  <li class="toctree-l1"><a class="reference internal" href="#">TdbcLib</a>
    <ul>
                <li class="toctree-l2"><a class="reference internal" href="../../TdbcLib/">TdbcLib</a>
                </li>
    </ul>
                  </li>
                  <li class="toctree-l1"><a class="reference internal" href="#">TdbcmysqlCmd</a>
    <ul>
                <li class="toctree-l2"><a class="reference internal" href="../../TdbcmysqlCmd/">TdbcmysqlCmd</a>
                </li>
    </ul>
                  </li>
                  <li class="toctree-l1"><a class="reference internal" href="#">TdbcodbcCmd</a>
    <ul>
                <li class="toctree-l2"><a class="reference internal" href="../../TdbcodbcCmd/">TdbcodbcCmd</a>
                </li>
    </ul>
                  </li>
                  <li class="toctree-l1"><a class="reference internal" href="#">TdbcpostgresCmd</a>
    <ul>
                <li class="toctree-l2"><a class="reference internal" href="../../TdbcpostgresCmd/">TdbcpostgresCmd</a>
                </li>
    </ul>
                  </li>
                  <li class="toctree-l1"><a class="reference internal" href="#">ThreadCmd</a>
    <ul>
                <li class="toctree-l2"><a class="reference internal" href="../../ThreadCmd/">ThreadCmd</a>
                </li>
    </ul>
                  </li>
                  <li class="toctree-l1"><a class="reference internal" href="#">TkCmd</a>
    <ul>
                <li class="toctree-l2"><a class="reference internal" href="../../TkCmd/">TkCmd</a>
                </li>
    </ul>
                  </li>
                  <li class="toctree-l1"><a class="reference internal" href="#">TkLib</a>
    <ul>
                <li class="toctree-l2"><a class="reference internal" href="../../TkLib/">TkLib</a>
                </li>
    </ul>
                  </li>
                  <li class="toctree-l1"><a class="reference internal" href="#">UserCmd</a>
    <ul>
                <li class="toctree-l2"><a class="reference internal" href="../../UserCmd/">UserCmd</a>
                </li>
    </ul>
                  </li>
              </ul>
      </div>
    </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap">
      <nav class="wy-nav-top" role="navigation" aria-label="Mobile navigation menu">
          <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
          <a href="../..">Tcl/Tk</a>
        
      </nav>
      <div class="wy-nav-content">
        <div class="rst-content"><div role="navigation" aria-label="breadcrumbs navigation">
  <ul class="wy-breadcrumbs">
    <li><a href="../.." class="icon icon-home" aria-label="Docs"></a> &raquo;</li>
      <li>NAME</li>
    <li class="wy-breadcrumbs-aside">
    </li>
  </ul>
  <hr/>
</div>
          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
            <div class="section" itemprop="articleBody">
              
                <h1 id="name">NAME</h1>
<p>tcltest - Test harness support code and utilities</p>
<h1 id="synopsis">SYNOPSIS</h1>
<p><strong>package require tcltest</strong> ?<strong>2.5</strong>?</p>
<p><strong>tcltest::test </strong><em>name description</em> ?<em>-option value ...</em>?
<strong>tcltest::test </strong><em>name description</em> ?<em>constraints</em>? <em>body result</em></p>
<p><strong>tcltest::loadTestedCommands</strong> <strong>tcltest::makeDirectory </strong><em>name</em>
?<em>directory</em>? <strong>tcltest::removeDirectory </strong><em>name</em> ?<em>directory</em>?
<strong>tcltest::makeFile </strong><em>contents name</em> ?<em>directory</em>?
<strong>tcltest::removeFile </strong><em>name</em> ?<em>directory</em>? <strong>tcltest::viewFile
</strong><em>name</em> ?<em>directory</em>? <strong>tcltest::cleanupTests
</strong>?<em>runningMultipleTests</em>? <strong>tcltest::runAllTests</strong></p>
<p><strong>tcltest::configure</strong> <strong>tcltest::configure </strong><em>-option</em>
<strong>tcltest::configure </strong><em>-option value</em> ?<em>-option value ...</em>?
<strong>tcltest::customMatch </strong><em>mode command</em> <strong>tcltest::testConstraint
</strong><em>constraint</em> ?<em>value</em>? <strong>tcltest::outputChannel </strong>?<em>channelID</em>?
<strong>tcltest::errorChannel </strong>?<em>channelID</em>? <strong>tcltest::interpreter
</strong>?<em>interp</em>?</p>
<p><strong>tcltest::debug </strong>?<em>level</em>? <strong>tcltest::errorFile </strong>?<em>filename</em>?
<strong>tcltest::limitConstraints </strong>?<em>boolean</em>? <strong>tcltest::loadFile
</strong>?<em>filename</em>? <strong>tcltest::loadScript </strong>?<em>script</em>? <strong>tcltest::match
</strong>?<em>patternList</em>? <strong>tcltest::matchDirectories </strong>?<em>patternList</em>?
<strong>tcltest::matchFiles </strong>?<em>patternList</em>? <strong>tcltest::outputFile
</strong>?<em>filename</em>? <strong>tcltest::preserveCore </strong>?<em>level</em>?
<strong>tcltest::singleProcess </strong>?<em>boolean</em>? <strong>tcltest::skip </strong>?<em>patternList</em>?
<strong>tcltest::skipDirectories </strong>?<em>patternList</em>? <strong>tcltest::skipFiles
</strong>?<em>patternList</em>? <strong>tcltest::temporaryDirectory </strong>?<em>directory</em>?
<strong>tcltest::testsDirectory </strong>?<em>directory</em>? <strong>tcltest::verbose </strong>?<em>level</em>?</p>
<p><strong>tcltest::test </strong><em>name description optionList</em> <strong>tcltest::bytestring
</strong><em>string</em> <strong>tcltest::normalizeMsg </strong><em>msg</em> <strong>tcltest::normalizePath
</strong><em>pathVar</em> <strong>tcltest::workingDirectory </strong>?<em>dir</em>?</p>
<h1 id="description">DESCRIPTION</h1>
<p>The <strong>tcltest</strong> package provides several utility commands useful in the
construction of test suites for code instrumented to be run by
evaluation of Tcl commands. Notably the built-in commands of the Tcl
library itself are tested by a test suite using the tcltest package.</p>
<p>All the commands provided by the <strong>tcltest</strong> package are defined in and
exported from the <strong>::tcltest</strong> namespace, as indicated in the
<strong>SYNOPSIS</strong> above. In the following sections, all commands will be
described by their simple names, in the interest of brevity.</p>
<p>The central command of <strong>tcltest</strong> is <strong>test</strong> that defines and runs a
test. Testing with <strong>test</strong> involves evaluation of a Tcl script and
comparing the result to an expected result, as configured and controlled
by a number of options. Several other commands provided by <strong>tcltest</strong>
govern the configuration of <strong>test</strong> and the collection of many <strong>test</strong>
commands into test suites.</p>
<p>See <strong>CREATING TEST SUITES WITH TCLTEST</strong> below for an extended example
of how to use the commands of <strong>tcltest</strong> to produce test suites for
your Tcl-enabled code.</p>
<h1 id="commands">COMMANDS</h1>
<p><strong>test</strong> <em>name description</em> ?<em>-option value ...</em>?</p>
<p>:   Defines and possibly runs a test with the name <em>name</em> and
    description <em>description</em>. The name and description of a test are
    used in messages reported by <strong>test</strong> during the test, as configured
    by the options of <strong>tcltest</strong>. The remaining <em>option value</em>
    arguments to <strong>test</strong> define the test, including the scripts to run,
    the conditions under which to run them, the expected result, and the
    means by which the expected and actual results should be compared.
    See <strong>TESTS</strong> below for a complete description of the valid options
    and how they define a test. The <strong>test</strong> command returns an empty
    string.</p>
<p><strong>test</strong> <em>name description</em> ?<em>constraints</em>? <em>body result</em></p>
<p>:   This form of <strong>test</strong> is provided to support test suites written for
    version 1 of the <strong>tcltest</strong> package, and also a simpler interface
    for a common usage. It is the same as All other options to <strong>test</strong>
    take their default values. When <em>constraints</em> is omitted, this form
    of <strong>test</strong> can be distinguished from the first because all
    <em>option</em>s begin with</p>
<p><strong>loadTestedCommands</strong></p>
<p>:   Evaluates in the caller\'s context the script specified by
    <strong>configure -load</strong> or <strong>configure -loadfile</strong>. Returns the result
    of that script evaluation, including any error raised by the script.
    Use this command and the related configuration options to provide
    the commands to be tested to the interpreter running the test suite.</p>
<p><strong>makeFile</strong> <em>contents name</em> ?<em>directory</em>?</p>
<p>:   Creates a file named <em>name</em> relative to directory <em>directory</em> and
    write <em>contents</em> to that file using the encoding <strong>encoding
    system</strong>. If <em>contents</em> does not end with a newline, a newline will
    be appended so that the file named <em>name</em> does end with a newline.
    Because the system encoding is used, this command is only suitable
    for making text files. The file will be removed by the next
    evaluation of <strong>cleanupTests</strong>, unless it is removed by
    <strong>removeFile</strong> first. The default value of <em>directory</em> is the
    directory <strong>configure -tmpdir</strong>. Returns the full path of the file
    created. Use this command to create any text file required by a test
    with contents as needed.</p>
<p><strong>removeFile</strong> <em>name</em> ?<em>directory</em>?</p>
<p>:   Forces the file referenced by <em>name</em> to be removed. This file name
    should be relative to <em>directory</em>. The default value of <em>directory</em>
    is the directory <strong>configure -tmpdir</strong>. Returns an empty string. Use
    this command to delete files created by <strong>makeFile</strong>.</p>
<p><strong>makeDirectory</strong> <em>name</em> ?<em>directory</em>?</p>
<p>:   Creates a directory named <em>name</em> relative to directory <em>directory</em>.
    The directory will be removed by the next evaluation of
    <strong>cleanupTests</strong>, unless it is removed by <strong>removeDirectory</strong> first.
    The default value of <em>directory</em> is the directory <strong>configure
    -tmpdir</strong>. Returns the full path of the directory created. Use this
    command to create any directories that are required to exist by a
    test.</p>
<p><strong>removeDirectory</strong> <em>name</em> ?<em>directory</em>?</p>
<p>:   Forces the directory referenced by <em>name</em> to be removed. This
    directory should be relative to <em>directory</em>. The default value of
    <em>directory</em> is the directory <strong>configure -tmpdir</strong>. Returns an empty
    string. Use this command to delete any directories created by
    <strong>makeDirectory</strong>.</p>
<p><strong>viewFile</strong> <em>file</em> ?<em>directory</em>?</p>
<p>:   Returns the contents of <em>file</em>, except for any final newline, just
    as <strong>read -nonewline</strong> would return. This file name should be
    relative to <em>directory</em>. The default value of <em>directory</em> is the
    directory <strong>configure -tmpdir</strong>. Use this command as a convenient
    way to turn the contents of a file generated by a test into the
    result of that test for matching against an expected result. The
    contents of the file are read using the system encoding, so its
    usefulness is limited to text files.</p>
<p><strong>cleanupTests</strong></p>
<p>:   Intended to clean up and summarize after several tests have been
    run. Typically called once per test file, at the end of the file
    after all tests have been completed. For best effectiveness, be sure
    that the <strong>cleanupTests</strong> is evaluated even if an error occurs
    earlier in the test file evaluation.</p>
<div class="language-text highlight"><pre><span></span><code>Prints statistics about the tests run and removes files that were
created by **makeDirectory** and **makeFile** since the last
**cleanupTests**. Names of files and directories in the directory
**configure -tmpdir** created since the last **cleanupTests**, but
not created by **makeFile** or **makeDirectory** are printed to
**outputChannel**. This command also restores the original shell
environment, as described by the global **env** array. Returns an
empty string.
</code></pre></div>
<p><strong>runAllTests</strong></p>
<p>:   This is a main command meant to run an entire suite of tests,
    spanning multiple files and/or directories, as governed by the
    configurable options of <strong>tcltest</strong>. See <strong>RUNNING ALL TESTS</strong> below
    for a complete description of the many variations possible with
    <strong>runAllTests</strong>.</p>
<h2 id="configuration-commands">CONFIGURATION COMMANDS</h2>
<p><strong>configure</strong></p>
<p>:   Returns the list of configurable options supported by <strong>tcltest</strong>.
    See <strong>CONFIGURABLE OPTIONS</strong> below for the full list of options,
    their valid values, and their effect on <strong>tcltest</strong> operations.</p>
<p><strong>configure </strong><em>option</em></p>
<p>:   Returns the current value of the supported configurable option
    <em>option</em>. Raises an error if <em>option</em> is not a supported
    configurable option.</p>
<p><strong>configure </strong><em>option value</em> ?<em>-option value ...</em>?</p>
<p>:   Sets the value of each configurable option <em>option</em> to the
    corresponding value <em>value</em>, in order. Raises an error if an
    <em>option</em> is not a supported configurable option, or if <em>value</em> is
    not a valid value for the corresponding <em>option</em>, or if a <em>value</em> is
    not provided. When an error is raised, the operation of
    <strong>configure</strong> is halted, and subsequent <em>option value</em> arguments are
    not processed.</p>
<div class="language-text highlight"><pre><span></span><code>If the environment variable **::env(TCLTEST_OPTIONS)** exists when
the **tcltest** package is loaded (by **package require**
**tcltest**) then its value is taken as a list of arguments to pass
to **configure**. This allows the default values of the
configuration options to be set by the environment.
</code></pre></div>
<p><strong>customMatch </strong><em>mode script</em></p>
<p>:   Registers <em>mode</em> as a new legal value of the <strong>-match</strong> option to
    <strong>test</strong>. When the <strong>-match </strong><em>mode</em> option is passed to <strong>test</strong>,
    the script <em>script</em> will be evaluated to compare the actual result
    of evaluating the body of the test to the expected result. To
    perform the match, the <em>script</em> is completed with two additional
    words, the expected result, and the actual result, and the completed
    script is evaluated in the global namespace. The completed script is
    expected to return a boolean value indicating whether or not the
    results match. The built-in matching modes of <strong>test</strong> are
    <strong>exact</strong>, <strong>glob</strong>, and <strong>regexp</strong>.</p>
<p><strong>testConstraint </strong><em>constraint</em> ?<em>boolean</em>?</p>
<p>:   Sets or returns the boolean value associated with the named
    <em>constraint</em>. See <strong>TEST CONSTRAINTS</strong> below for more information.</p>
<p><strong>interpreter</strong> ?<em>executableName</em>?</p>
<p>:   Sets or returns the name of the executable to be <strong>exec</strong>ed by
    <strong>runAllTests</strong> to run each test file when <strong>configure -singleproc</strong>
    is false. The default value for <strong>interpreter</strong> is the name of the
    currently running program as returned by <strong>info nameofexecutable</strong>.</p>
<p><strong>outputChannel</strong> ?<em>channelID</em>?</p>
<p>:   Sets or returns the output channel ID. This defaults to <strong>stdout</strong>.
    Any test that prints test related output should send that output to
    <strong>outputChannel</strong> rather than letting that output default to
    <strong>stdout</strong>.</p>
<p><strong>errorChannel</strong> ?<em>channelID</em>?</p>
<p>:   Sets or returns the error channel ID. This defaults to <strong>stderr</strong>.
    Any test that prints error messages should send that output to
    <strong>errorChannel</strong> rather than printing directly to <strong>stderr</strong>.</p>
<h2 id="shortcut-configuration-commands">SHORTCUT CONFIGURATION COMMANDS</h2>
<p><strong>debug</strong> ?<em>level</em>?</p>
<p>:   Same as</p>
<p><strong>errorFile</strong> ?<em>filename</em>?</p>
<p>:   Same as</p>
<p><strong>limitConstraints</strong> ?<em>boolean</em>?</p>
<p>:   Same as</p>
<p><strong>loadFile</strong> ?<em>filename</em>?</p>
<p>:   Same as</p>
<p><strong>loadScript</strong> ?<em>script</em>?</p>
<p>:   Same as</p>
<p><strong>match</strong> ?<em>patternList</em>?</p>
<p>:   Same as</p>
<p><strong>matchDirectories</strong> ?<em>patternList</em>?</p>
<p>:   Same as</p>
<p><strong>matchFiles</strong> ?<em>patternList</em>?</p>
<p>:   Same as</p>
<p><strong>outputFile</strong> ?<em>filename</em>?</p>
<p>:   Same as</p>
<p><strong>preserveCore</strong> ?<em>level</em>?</p>
<p>:   Same as</p>
<p><strong>singleProcess</strong> ?<em>boolean</em>?</p>
<p>:   Same as</p>
<p><strong>skip</strong> ?<em>patternList</em>?</p>
<p>:   Same as</p>
<p><strong>skipDirectories</strong> ?<em>patternList</em>?</p>
<p>:   Same as</p>
<p><strong>skipFiles</strong> ?<em>patternList</em>?</p>
<p>:   Same as</p>
<p><strong>temporaryDirectory</strong> ?<em>directory</em>?</p>
<p>:   Same as</p>
<p><strong>testsDirectory</strong> ?<em>directory</em>?</p>
<p>:   Same as</p>
<p><strong>verbose</strong> ?<em>level</em>?</p>
<p>:   Same as</p>
<h2 id="other-commands">OTHER COMMANDS</h2>
<p>The remaining commands provided by <strong>tcltest</strong> have better alternatives
provided by <strong>tcltest</strong> or <strong>Tcl</strong> itself. They are retained to support
existing test suites, but should be avoided in new code.</p>
<p><strong>test</strong> <em>name description optionList</em></p>
<p>:   This form of <strong>test</strong> was provided to enable passing many options
    spanning several lines to <strong>test</strong> as a single argument quoted by
    braces, rather than needing to backslash quote the newlines between
    arguments to <strong>test</strong>. The <em>optionList</em> argument is expected to be a
    list with an even number of elements representing <em>option</em> and
    <em>value</em> arguments to pass to <strong>test</strong>. However, these values are not
    passed directly, as in the alternate forms of <strong>switch</strong>. Instead,
    this form makes an unfortunate attempt to overthrow Tcl\'s
    substitution rules by performing substitutions on some of the list
    elements as an attempt to implement a interpretation of a
    brace-enclosed The result is nearly impossible to document clearly,
    and for that reason this form is not recommended. See the examples
    in <strong>CREATING TEST SUITES WITH TCLTEST</strong> below to see that this form
    is really not necessary to avoid backslash-quoted newlines. If you
    insist on using this form, examine the source code of <strong>tcltest</strong> if
    you want to know the substitution details, or just enclose the third
    through last argument to <strong>test</strong> in braces and hope for the best.</p>
<p><strong>workingDirectory</strong> ?<em>directoryName</em>?</p>
<p>:   Sets or returns the current working directory when the test suite is
    running. The default value for workingDirectory is the directory in
    which the test suite was launched. The Tcl commands <strong>cd</strong> and
    <strong>pwd</strong> are sufficient replacements.</p>
<p><strong>normalizeMsg </strong><em>msg</em></p>
<p>:   Returns the result of removing the newlines from <em>msg</em>, where is
    rather imprecise. Tcl offers plenty of string processing commands to
    modify strings as you wish, and <strong>customMatch</strong> allows flexible
    matching of actual and expected results.</p>
<p><strong>normalizePath </strong><em>pathVar</em></p>
<p>:   Resolves symlinks in a path, thus creating a path without internal
    redirection. It is assumed that <em>pathVar</em> is absolute. <em>pathVar</em> is
    modified in place. The Tcl command <strong>file normalize</strong> is a
    sufficient replacement.</p>
<p><strong>bytestring </strong><em>string</em></p>
<p>:   Construct a string that consists of the requested sequence of bytes,
    as opposed to a string of properly formed UTF-8 characters using the
    value supplied in <em>string</em>. This allows the tester to create
    denormalized or improperly formed strings to pass to C procedures
    that are supposed to accept strings with embedded NULL types and
    confirm that a string result has a certain pattern of bytes. This is
    exactly equivalent to the Tcl command <strong>encoding convertfrom</strong>
    <strong>identity</strong>.</p>
<h1 id="tests">TESTS</h1>
<p>The <strong>test</strong> command is the heart of the <strong>tcltest</strong> package. Its
essential function is to evaluate a Tcl script and compare the result
with an expected result. The options of <strong>test</strong> define the test script,
the environment in which to evaluate it, the expected result, and how
the compare the actual result to the expected result. Some configuration
options of <strong>tcltest</strong> also influence how <strong>test</strong> operates.</p>
<p>The valid options for <strong>test</strong> are summarized:</p>
<div class="language-text highlight"><pre><span></span><code>test name description
        ?-constraints keywordList|expression?
        ?-setup setupScript?
        ?-body testScript?
        ?-cleanup cleanupScript?
        ?-result expectedAnswer?
        ?-output expectedOutput?
        ?-errorOutput expectedError?
        ?-returnCodes codeList?
        ?-errorCode expectedErrorCode?
        ?-match mode?
</code></pre></div>
<p>The <em>name</em> may be any string. It is conventional to choose a <em>name</em>
according to the pattern:</p>
<div class="language-text highlight"><pre><span></span><code>target-majorNum.minorNum
</code></pre></div>
<p>For white-box (regression) tests, the target should be the name of the C
function or Tcl procedure being tested. For black-box tests, the target
should be the name of the feature being tested. Some conventions call
for the names of black-box tests to have the suffix <strong>_bb</strong>. Related
tests should share a major number. As a test suite evolves, it is best
to have the same test name continue to correspond to the same test, so
that it remains meaningful to say things like</p>
<p>During evaluation of <strong>test</strong>, the <em>name</em> will be compared to the lists
of string matching patterns returned by <strong>configure -match</strong>, and
<strong>configure -skip</strong>. The test will be run only if <em>name</em> matches any of
the patterns from <strong>configure -match</strong> and matches none of the patterns
from <strong>configure -skip</strong>.</p>
<p>The <em>description</em> should be a short textual description of the test. The
<em>description</em> is included in output produced by the test, typically test
failure messages. Good <em>description</em> values should briefly explain the
purpose of the test to users of a test suite. The name of a Tcl or C
function being tested should be included in the description for
regression tests. If the test case exists to reproduce a bug, include
the bug ID in the description.</p>
<p>Valid attributes and associated values are:</p>
<p><strong>-constraints </strong><em>keywordList</em>|<em>expression</em></p>
<p>:   The optional <strong>-constraints</strong> attribute can be list of one or more
    keywords or an expression. If the <strong>-constraints</strong> value is a list
    of keywords, each of these keywords should be the name of a
    constraint defined by a call to <strong>testConstraint</strong>. If any of the
    listed constraints is false or does not exist, the test is skipped.
    If the <strong>-constraints</strong> value is an expression, that expression is
    evaluated. If the expression evaluates to true, then the test is
    run. Note that the expression form of <strong>-constraints</strong> may interfere
    with the operation of <strong>configure -constraints</strong> and <strong>configure
    -limitconstraints</strong>, and is not recommended. Appropriate constraints
    should be added to any tests that should not always be run. That is,
    conditional evaluation of a test should be accomplished by the
    <strong>-constraints</strong> option, not by conditional evaluation of <strong>test</strong>.
    In that way, the same number of tests are always reported by the
    test suite, though the number skipped may change based on the
    testing environment. The default value is an empty list. See <strong>TEST
    CONSTRAINTS</strong> below for a list of built-in constraints and
    information on how to add your own constraints.</p>
<p><strong>-setup </strong><em>script</em></p>
<p>:   The optional <strong>-setup</strong> attribute indicates a <em>script</em> that will be
    run before the script indicated by the <strong>-body</strong> attribute. If
    evaluation of <em>script</em> raises an error, the test will fail. The
    default value is an empty script.</p>
<p><strong>-body </strong><em>script</em></p>
<p>:   The <strong>-body</strong> attribute indicates the <em>script</em> to run to carry out
    the test, which must return a result that can be checked for
    correctness. If evaluation of <em>script</em> raises an error, the test
    will fail (unless the <strong>-returnCodes</strong> option is used to state that
    an error is expected). The default value is an empty script.</p>
<p><strong>-cleanup </strong><em>script</em></p>
<p>:   The optional <strong>-cleanup</strong> attribute indicates a <em>script</em> that will
    be run after the script indicated by the <strong>-body</strong> attribute. If
    evaluation of <em>script</em> raises an error, the test will fail. The
    default value is an empty script.</p>
<p><strong>-match </strong><em>mode</em></p>
<p>:   The <strong>-match</strong> attribute determines how expected answers supplied by
    <strong>-result</strong>, <strong>-output</strong>, and <strong>-errorOutput</strong> are compared. Valid
    values for <em>mode</em> are <strong>regexp</strong>, <strong>glob</strong>, <strong>exact</strong>, and any value
    registered by a prior call to <strong>customMatch</strong>. The default value is
    <strong>exact</strong>.</p>
<p><strong>-result </strong><em>expectedValue</em></p>
<p>:   The <strong>-result</strong> attribute supplies the <em>expectedValue</em> against which
    the return value from script will be compared. The default value is
    an empty string.</p>
<p><strong>-output </strong><em>expectedValue</em></p>
<p>:   The <strong>-output</strong> attribute supplies the <em>expectedValue</em> against which
    any output sent to <strong>stdout</strong> or <strong>outputChannel</strong> during evaluation
    of the script(s) will be compared. Note that only output printed
    using the global <strong>puts</strong> command is used for comparison. If
    <strong>-output</strong> is not specified, output sent to <strong>stdout</strong> and
    <strong>outputChannel</strong> is not processed for comparison.</p>
<p><strong>-errorOutput </strong><em>expectedValue</em></p>
<p>:   The <strong>-errorOutput</strong> attribute supplies the <em>expectedValue</em> against
    which any output sent to <strong>stderr</strong> or <strong>errorChannel</strong> during
    evaluation of the script(s) will be compared. Note that only output
    printed using the global <strong>puts</strong> command is used for comparison. If
    <strong>-errorOutput</strong> is not specified, output sent to <strong>stderr</strong> and
    <strong>errorChannel</strong> is not processed for comparison.</p>
<p><strong>-returnCodes </strong><em>expectedCodeList</em></p>
<p>:   The optional <strong>-returnCodes</strong> attribute supplies <em>expectedCodeList</em>,
    a list of return codes that may be accepted from evaluation of the
    <strong>-body</strong> script. If evaluation of the <strong>-body</strong> script returns a
    code not in the <em>expectedCodeList</em>, the test fails. All return codes
    known to <strong>return</strong>, in both numeric and symbolic form, including
    extended return codes, are acceptable elements in the
    <em>expectedCodeList</em>. Default value is</p>
<p><strong>-errorCode </strong><em>expectedErrorCode</em></p>
<p>:   The optional <strong>-errorCode</strong> attribute supplies <em>expectedErrorCode</em>,
    a glob pattern that should match the error code reported from
    evaluation of the <strong>-body</strong> script. If evaluation of the <strong>-body</strong>
    script returns a code not matching <em>expectedErrorCode</em>, the test
    fails. Default value is If <strong>-returnCodes</strong> does not include
    <strong>error</strong> it is set to <strong>error</strong>.</p>
<p>To pass, a test must successfully evaluate its <strong>-setup</strong>, <strong>-body</strong>,
and <strong>-cleanup</strong> scripts. The return code of the <strong>-body</strong> script and
its result must match expected values, and if specified, output and
error data from the test must match expected <strong>-output</strong> and
<strong>-errorOutput</strong> values. If any of these conditions are not met, then
the test fails. Note that all scripts are evaluated in the context of
the caller of <strong>test</strong>.</p>
<p>As long as <strong>test</strong> is called with valid syntax and legal values for all
attributes, it will not raise an error. Test failures are instead
reported as output written to <strong>outputChannel</strong>. In default operation, a
successful test produces no output. The output messages produced by
<strong>test</strong> are controlled by the <strong>configure -verbose</strong> option as
described in <strong>CONFIGURABLE OPTIONS</strong> below. Any output produced by the
test scripts themselves should be produced using <strong>puts</strong> to
<strong>outputChannel</strong> or <strong>errorChannel</strong>, so that users of the test suite
may easily capture output with the <strong>configure -outfile</strong> and
<strong>configure -errfile</strong> options, and so that the <strong>-output</strong> and
<strong>-errorOutput</strong> attributes work properly.</p>
<h2 id="test-constraints">TEST CONSTRAINTS</h2>
<p>Constraints are used to determine whether or not a test should be
skipped. Each constraint has a name, which may be any string, and a
boolean value. Each <strong>test</strong> has a <strong>-constraints</strong> value which is a
list of constraint names. There are two modes of constraint control.
Most frequently, the default mode is used, indicated by a setting of
<strong>configure -limitconstraints</strong> to false. The test will run only if all
constraints in the list are true-valued. Thus, the <strong>-constraints</strong>
option of <strong>test</strong> is a convenient, symbolic way to define any
conditions required for the test to be possible or meaningful. For
example, a <strong>test</strong> with <strong>-constraints unix</strong> will only be run if the
constraint <strong>unix</strong> is true, which indicates the test suite is being run
on a Unix platform.</p>
<p>Each <strong>test</strong> should include whatever <strong>-constraints</strong> are required to
constrain it to run only where appropriate. Several constraints are
pre-defined in the <strong>tcltest</strong> package, listed below. The registration
of user-defined constraints is performed by the <strong>testConstraint</strong>
command. User-defined constraints may appear within a test file, or
within the script specified by the <strong>configure -load</strong> or <strong>configure
-loadfile</strong> options.</p>
<p>The following is a list of constraints pre-defined by the <strong>tcltest</strong>
package itself:</p>
<p><em>singleTestInterp</em></p>
<p>:   This test can only be run if all test files are sourced into a
    single interpreter.</p>
<p><em>unix</em></p>
<p>:   This test can only be run on any Unix platform.</p>
<p><em>win</em></p>
<p>:   This test can only be run on any Windows platform.</p>
<p><em>nt</em></p>
<p>:   This test can only be run on any Windows NT platform.</p>
<p><em>mac</em></p>
<p>:   This test can only be run on any Mac platform.</p>
<p><em>unixOrWin</em></p>
<p>:   This test can only be run on a Unix or Windows platform.</p>
<p><em>macOrWin</em></p>
<p>:   This test can only be run on a Mac or Windows platform.</p>
<p><em>macOrUnix</em></p>
<p>:   This test can only be run on a Mac or Unix platform.</p>
<p><em>tempNotWin</em></p>
<p>:   This test can not be run on Windows. This flag is used to
    temporarily disable a test.</p>
<p><em>tempNotMac</em></p>
<p>:   This test can not be run on a Mac. This flag is used to temporarily
    disable a test.</p>
<p><em>unixCrash</em></p>
<p>:   This test crashes if it is run on Unix. This flag is used to
    temporarily disable a test.</p>
<p><em>winCrash</em></p>
<p>:   This test crashes if it is run on Windows. This flag is used to
    temporarily disable a test.</p>
<p><em>macCrash</em></p>
<p>:   This test crashes if it is run on a Mac. This flag is used to
    temporarily disable a test.</p>
<p><em>emptyTest</em></p>
<p>:   This test is empty, and so not worth running, but it remains as a
    place-holder for a test to be written in the future. This constraint
    has value false to cause tests to be skipped unless the user
    specifies otherwise.</p>
<p><em>knownBug</em></p>
<p>:   This test is known to fail and the bug is not yet fixed. This
    constraint has value false to cause tests to be skipped unless the
    user specifies otherwise.</p>
<p><em>nonPortable</em></p>
<p>:   This test can only be run in some known development environment.
    Some tests are inherently non-portable because they depend on things
    like word length, file system configuration, window manager, etc.
    This constraint has value false to cause tests to be skipped unless
    the user specifies otherwise.</p>
<p><em>userInteraction</em></p>
<p>:   This test requires interaction from the user. This constraint has
    value false to causes tests to be skipped unless the user specifies
    otherwise.</p>
<p><em>interactive</em></p>
<p>:   This test can only be run in if the interpreter is in interactive
    mode (when the global tcl_interactive variable is set to 1).</p>
<p><em>nonBlockFiles</em></p>
<p>:   This test can only be run if platform supports setting files into
    nonblocking mode.</p>
<p><em>asyncPipeClose</em></p>
<p>:   This test can only be run if platform supports async flush and async
    close on a pipe.</p>
<p><em>unixExecs</em></p>
<p>:   This test can only be run if this machine has Unix-style commands
    <strong>cat</strong>, <strong>echo</strong>, <strong>sh</strong>, <strong>wc</strong>, <strong>rm</strong>, <strong>sleep</strong>, <strong>fgrep</strong>,
    <strong>ps</strong>, <strong>chmod</strong>, and <strong>mkdir</strong> available.</p>
<p><em>hasIsoLocale</em></p>
<p>:   This test can only be run if can switch to an ISO locale.</p>
<p><em>root</em></p>
<p>:   This test can only run if Unix user is root.</p>
<p><em>notRoot</em></p>
<p>:   This test can only run if Unix user is not root.</p>
<p><em>eformat</em></p>
<p>:   This test can only run if app has a working version of sprintf with
    respect to the format of floating-point numbers.</p>
<p><em>stdio</em></p>
<p>:   This test can only be run if <strong>interpreter</strong> can be <strong>open</strong>ed as a
    pipe.</p>
<p>The alternative mode of constraint control is enabled by setting
<strong>configure -limitconstraints</strong> to true. With that configuration
setting, all existing constraints other than those in the constraint
list returned by <strong>configure -constraints</strong> are set to false. When the
value of <strong>configure -constraints</strong> is set, all those constraints are
set to true. The effect is that when both options <strong>configure
-constraints</strong> and <strong>configure -limitconstraints</strong> are in use, only
those tests including only constraints from the <strong>configure
-constraints</strong> list are run; all others are skipped. For example, one
might set up a configuration with</p>
<div class="language-text highlight"><pre><span></span><code>configure -constraints knownBug \
          -limitconstraints true \
          -verbose pass
</code></pre></div>
<p>to run exactly those tests that exercise known bugs, and discover
whether any of them pass, indicating the bug had been fixed.</p>
<h2 id="running-all-tests">RUNNING ALL TESTS</h2>
<p>The single command <strong>runAllTests</strong> is evaluated to run an entire test
suite, spanning many files and directories. The configuration options of
<strong>tcltest</strong> control the precise operations. The <strong>runAllTests</strong> command
begins by printing a summary of its configuration to <strong>outputChannel</strong>.</p>
<p>Test files to be evaluated are sought in the directory <strong>configure
-testdir</strong>. The list of files in that directory that match any of the
patterns in <strong>configure -file</strong> and match none of the patterns in
<strong>configure -notfile</strong> is generated and sorted. Then each file will be
evaluated in turn. If <strong>configure -singleproc</strong> is true, then each file
will be <strong>source</strong>d in the caller\'s context. If it is false, then a
copy of <strong>interpreter</strong> will be <strong>exec</strong>\'d to evaluate each file. The
multi-process operation is useful when testing can cause errors so
severe that a process terminates. Although such an error may terminate a
child process evaluating one file, the main process can continue with
the rest of the test suite. In multi-process operation, the
configuration of <strong>tcltest</strong> in the main process is passed to the child
processes as command line arguments, with the exception of <strong>configure
-outfile</strong>. The <strong>runAllTests</strong> command in the main process collects all
output from the child processes and collates their results into one main
report. Any reports of individual test failures, or messages requested
by a <strong>configure -verbose</strong> setting are passed directly on to
<strong>outputChannel</strong> by the main process.</p>
<p>After evaluating all selected test files, a summary of the results is
printed to <strong>outputChannel</strong>. The summary includes the total number of
<strong>test</strong>s evaluated, broken down into those skipped, those passed, and
those failed. The summary also notes the number of files evaluated, and
the names of any files with failing tests or errors. A list of the
constraints that caused tests to be skipped, and the number of tests
skipped for each is also printed. Also, messages are printed if it
appears that evaluation of a test file has caused any temporary files to
be left behind in <strong>configure -tmpdir</strong>.</p>
<p>Having completed and summarized all selected test files, <strong>runAllTests</strong>
then recursively acts on subdirectories of <strong>configure -testdir</strong>. All
subdirectories that match any of the patterns in <strong>configure
-relateddir</strong> and do not match any of the patterns in <strong>configure
-asidefromdir</strong> are examined. If a file named <strong>all.tcl</strong> is found in
such a directory, it will be <strong>source</strong>d in the caller\'s context.
Whether or not an examined directory contains an <strong>all.tcl</strong> file, its
subdirectories are also scanned against the <strong>configure -relateddir</strong>
and <strong>configure -asidefromdir</strong> patterns. In this way, many directories
in a directory tree can have all their test files evaluated by a single
<strong>runAllTests</strong> command.</p>
<h1 id="configurable-options">CONFIGURABLE OPTIONS</h1>
<p>The <strong>configure</strong> command is used to set and query the configurable
options of <strong>tcltest</strong>. The valid options are:</p>
<p><strong>-singleproc </strong><em>boolean</em></p>
<p>:   Controls whether or not <strong>runAllTests</strong> spawns a child process for
    each test file. No spawning when <em>boolean</em> is true. Default value is
    false.</p>
<p><strong>-debug </strong><em>level</em></p>
<p>:   Sets the debug level to <em>level</em>, an integer value indicating how
    much debugging information should be printed to <strong>stdout</strong>. Note
    that debug messages always go to <strong>stdout</strong>, independent of the
    value of <strong>configure -outfile</strong>. Default value is 0. Levels are
    defined as:</p>
<div class="language-text highlight"><pre><span></span><code>0.  Do not display any debug information.

1.  Display information regarding whether a test is skipped because
    it does not match any of the tests that were specified using by
    **configure -match** (userSpecifiedNonMatch) or matches any of
    the tests specified by **configure -skip** (userSpecifiedSkip).
    Also print warnings about possible lack of cleanup or balance in
    test files. Also print warnings about any re-use of test names.

2.  Display the flag array parsed by the command line processor, the
    contents of the global **env** array, and all user-defined
    variables that exist in the current namespace as they are used.

3.  Display information regarding what individual procs in the test
    harness are doing.
</code></pre></div>
<p><strong>-verbose </strong><em>level</em></p>
<p>:   Sets the type of output verbosity desired to <em>level</em>, a list of zero
    or more of the elements <strong>body</strong>, <strong>pass</strong>, <strong>skip</strong>, <strong>start</strong>,
    <strong>error</strong>, <strong>line</strong>, <strong>msec</strong> and <strong>usec</strong>. Default value is Levels
    are defined as:</p>
<div class="language-text highlight"><pre><span></span><code>body (b)

:   Display the body of failed tests

pass (p)

:   Print output when a test passes

skip (s)

:   Print output when a test is skipped

start (t)

:   Print output whenever a test starts

error (e)

:   Print errorInfo and errorCode, if they exist, when a test return
    code does not match its expected return code

line (l)

:   Print source file line information of failed tests

msec (m)

:   Print each test\&#39;s execution time in milliseconds

usec (u)

:   Print each test\&#39;s execution time in microseconds

Note that the **msec** and **usec** verbosity levels are provided as
indicative measures only. They do not tackle the problem of
repeatibility which should be considered in performance tests or
benchmarks. To use these verbosity levels to thoroughly track
performance degradations, consider wrapping your test bodies with
**time** commands.

The single letter abbreviations noted above are also recognized so
that is the same as
</code></pre></div>
<p><strong>-preservecore </strong><em>level</em></p>
<p>:   Sets the core preservation level to <em>level</em>. This level determines
    how stringent checks for core files are. Default value is 0. Levels
    are defined as:</p>
<div class="language-text highlight"><pre><span></span><code>0.  No checking --- do not check for core files at the end of each
    test command, but do check for them in **runAllTests** after all
    test files have been evaluated.

1.  Also check for core files at the end of each **test** command.

2.  Check for core files at all times described above, and save a
    copy of each core file produced in **configure -tmpdir**.
</code></pre></div>
<p><strong>-limitconstraints </strong><em>boolean</em></p>
<p>:   Sets the mode by which <strong>test</strong> honors constraints as described in
    <strong>TESTS</strong> above. Default value is false.</p>
<p><strong>-constraints </strong><em>list</em></p>
<p>:   Sets all the constraints in <em>list</em> to true. Also used in combination
    with <strong>configure -limitconstraints true</strong> to control an alternative
    constraint mode as described in <strong>TESTS</strong> above. Default value is an
    empty list.</p>
<p><strong>-tmpdir </strong><em>directory</em></p>
<p>:   Sets the temporary directory to be used by <strong>makeFile</strong>,
    <strong>makeDirectory</strong>, <strong>viewFile</strong>, <strong>removeFile</strong>, and
    <strong>removeDirectory</strong> as the default directory where temporary files
    and directories created by test files should be created. Default
    value is <strong>workingDirectory</strong>.</p>
<p><strong>-testdir </strong><em>directory</em></p>
<p>:   Sets the directory searched by <strong>runAllTests</strong> for test files and
    subdirectories. Default value is <strong>workingDirectory</strong>.</p>
<p><strong>-file </strong><em>patternList</em></p>
<p>:   Sets the list of patterns used by <strong>runAllTests</strong> to determine what
    test files to evaluate. Default value is</p>
<p><strong>-notfile </strong><em>patternList</em></p>
<p>:   Sets the list of patterns used by <strong>runAllTests</strong> to determine what
    test files to skip. Default value is so that any SCCS lock files are
    skipped.</p>
<p><strong>-relateddir </strong><em>patternList</em></p>
<p>:   Sets the list of patterns used by <strong>runAllTests</strong> to determine what
    subdirectories to search for an <strong>all.tcl</strong> file. Default value is</p>
<p><strong>-asidefromdir </strong><em>patternList</em></p>
<p>:   Sets the list of patterns used by <strong>runAllTests</strong> to determine what
    subdirectories to skip when searching for an <strong>all.tcl</strong> file.
    Default value is an empty list.</p>
<p><strong>-match </strong><em>patternList</em></p>
<p>:   Set the list of patterns used by <strong>test</strong> to determine whether a
    test should be run. Default value is</p>
<p><strong>-skip </strong><em>patternList</em></p>
<p>:   Set the list of patterns used by <strong>test</strong> to determine whether a
    test should be skipped. Default value is an empty list.</p>
<p><strong>-load </strong><em>script</em></p>
<p>:   Sets a script to be evaluated by <strong>loadTestedCommands</strong>. Default
    value is an empty script.</p>
<p><strong>-loadfile </strong><em>filename</em></p>
<p>:   Sets the filename from which to read a script to be evaluated by
    <strong>loadTestedCommands</strong>. This is an alternative to <strong>-load</strong>. They
    cannot be used together.</p>
<p><strong>-outfile </strong><em>filename</em></p>
<p>:   Sets the file to which all output produced by tcltest should be
    written. A file named <em>filename</em> will be <strong>open</strong>ed for writing, and
    the resulting channel will be set as the value of <strong>outputChannel</strong>.</p>
<p><strong>-errfile </strong><em>filename</em></p>
<p>:   Sets the file to which all error output produced by tcltest should
    be written. A file named <em>filename</em> will be <strong>open</strong>ed for writing,
    and the resulting channel will be set as the value of
    <strong>errorChannel</strong>.</p>
<h1 id="creating-test-suites-with-tcltest">CREATING TEST SUITES WITH TCLTEST</h1>
<p>The fundamental element of a test suite is the individual <strong>test</strong>
command. We begin with several examples.</p>
<p>[1]</p>
<p>:   Test of a script that returns normally.</p>
<div class="language-text highlight"><pre><span></span><code>    test example-1.0 {normal return} {
        format %s value
    } value
</code></pre></div>
<p>[2]</p>
<p>:   Test of a script that requires context setup and cleanup. Note the
    bracing and indenting style that avoids any need for line
    continuation.</p>
<div class="language-text highlight"><pre><span></span><code>    test example-1.1 {test file existence} -setup {
        set file [makeFile {} test]
    } -body {
        file exists $file
    } -cleanup {
        removeFile test
    } -result 1
</code></pre></div>
<p>[3]</p>
<p>:   Test of a script that raises an error.</p>
<div class="language-text highlight"><pre><span></span><code>    test example-1.2 {error return} -body {
        error message
    } -returnCodes error -result message
</code></pre></div>
<p>[4]</p>
<p>:   Test with a constraint.</p>
<div class="language-text highlight"><pre><span></span><code>    test example-1.3 {user owns created files} -constraints {
        unix
    } -setup {
        set file [makeFile {} test]
    } -body {
        file attributes $file -owner
    } -cleanup {
        removeFile test
    } -result $::tcl_platform(user)
</code></pre></div>
<p>At the next higher layer of organization, several <strong>test</strong> commands are
gathered together into a single test file. Test files should have names
with the extension, because that is the default pattern used by
<strong>runAllTests</strong> to find test files. It is a good rule of thumb to have
one test file for each source code file of your project. It is good
practice to edit the test file and the source code file together,
keeping tests synchronized with code changes.</p>
<p>Most of the code in the test file should be the <strong>test</strong> commands. Use
constraints to skip tests, rather than conditional evaluation of
<strong>test</strong>.</p>
<p>[5]</p>
<p>:   Recommended system for writing conditional tests, using constraints
    to guard:</p>
<div class="language-text highlight"><pre><span></span><code>    testConstraint X [expr $myRequirement]
    test goodConditionalTest {} X {
        # body
    } result
</code></pre></div>
<p>[6]</p>
<p>:   Discouraged system for writing conditional tests, using <strong>if</strong> to
    guard:</p>
<div class="language-text highlight"><pre><span></span><code>    if $myRequirement {
        test badConditionalTest {} {
            #body
        } result
    }
</code></pre></div>
<p>Use the <strong>-setup</strong> and <strong>-cleanup</strong> options to establish and release all
context requirements of the test body. Do not make tests depend on prior
tests in the file. Those prior tests might be skipped. If several
consecutive tests require the same context, the appropriate setup and
cleanup scripts may be stored in variable for passing to each tests
<strong>-setup</strong> and <strong>-cleanup</strong> options. This is a better solution than
performing setup outside of <strong>test</strong> commands, because the setup will
only be done if necessary, and any errors during setup will be reported,
and not cause the test file to abort.</p>
<p>A test file should be able to be combined with other test files and not
interfere with them, even when <strong>configure -singleproc 1</strong> causes all
files to be evaluated in a common interpreter. A simple way to achieve
this is to have your tests define all their commands and variables in a
namespace that is deleted when the test file evaluation is complete. A
good namespace to use is a child namespace <strong>test</strong> of the namespace of
the module you are testing.</p>
<p>A test file should also be able to be evaluated directly as a script,
not depending on being called by a main <strong>runAllTests</strong>. This means that
each test file should process command line arguments to give the tester
all the configuration control that <strong>tcltest</strong> provides.</p>
<p>After all <strong>test</strong>s in a test file, the command <strong>cleanupTests</strong> should
be called.</p>
<p>[7]</p>
<p>:   Here is a sketch of a sample test file illustrating those points:</p>
<div class="language-text highlight"><pre><span></span><code>    package require tcltest 2.5
    eval ::tcltest::configure $argv
    package require example
    namespace eval ::example::test {
        namespace import ::tcltest::*
        testConstraint X [expr {...}]
        variable SETUP {#common setup code}
        variable CLEANUP {#common cleanup code}
        test example-1 {} -setup $SETUP -body {
            # First test
        } -cleanup $CLEANUP -result {...}
        test example-2 {} -constraints X -setup $SETUP -body {
            # Second test; constrained
        } -cleanup $CLEANUP -result {...}
        test example-3 {} {
            # Third test; no context required
        } {...}
        cleanupTests
    }
    namespace delete ::example::test
</code></pre></div>
<p>The next level of organization is a full test suite, made up of several
test files. One script is used to control the entire suite. The basic
function of this script is to call <strong>runAllTests</strong> after doing any
necessary setup. This script is usually named <strong>all.tcl</strong> because that
is the default name used by <strong>runAllTests</strong> when combining multiple test
suites into one testing run.</p>
<p>[8]</p>
<p>:   Here is a sketch of a sample test suite main script:</p>
<div class="language-text highlight"><pre><span></span><code>    package require Tcl 8.6
    package require tcltest 2.5
    package require example
    ::tcltest::configure -testdir \
            [file dirname [file normalize [info script]]]
    eval ::tcltest::configure $argv
    ::tcltest::runAllTests
</code></pre></div>
<h1 id="compatibility">COMPATIBILITY</h1>
<p>A number of commands and variables in the <strong>::tcltest</strong> namespace
provided by earlier releases of <strong>tcltest</strong> have not been documented
here. They are no longer part of the supported public interface of
<strong>tcltest</strong> and should not be used in new test suites. However, to
continue to support existing test suites written to the older interface
specifications, many of those deprecated commands and variables still
work as before. For example, in many circumstances, <strong>configure</strong> will
be automatically called shortly after <strong>package require</strong> <strong>tcltest
2.1</strong> succeeds with arguments from the variable <strong>::argv</strong>. This is to
support test suites that depend on the old behavior that <strong>tcltest</strong> was
automatically configured from command line arguments. New test files
should not depend on this, but should explicitly include</p>
<div class="language-text highlight"><pre><span></span><code>eval ::tcltest::configure $::argv
</code></pre></div>
<p>or</p>
<div class="language-text highlight"><pre><span></span><code>::tcltest::configure {*}$::argv
</code></pre></div>
<p>to establish a configuration from command line arguments.</p>
<h1 id="known-issues">KNOWN ISSUES</h1>
<p>There are two known issues related to nested evaluations of <strong>test</strong>.
The first issue relates to the stack level in which test scripts are
executed. Tests nested within other tests may be executed at the same
stack level as the outermost test. For example, in the following code:</p>
<div class="language-text highlight"><pre><span></span><code>test level-1.1 {level 1} {
    -body {
        test level-2.1 {level 2} {
        }
    }
}
</code></pre></div>
<p>any script executed in level-2.1 may be executed at the same stack level
as the script defined for level-1.1.</p>
<p>In addition, while two <strong>test</strong>s have been run, results will only be
reported by <strong>cleanupTests</strong> for tests at the same level as test
level-1.1. However, test results for all tests run prior to level-1.1
will be available when test level-2.1 runs. What this means is that if
you try to access the test results for test level-2.1, it will may say
that tests have run, tests have been skipped, tests have passed and
tests have failed, where and refer to tests that were run at the same
test level as test level-1.1.</p>
<p>Implementation of output and error comparison in the test command
depends on usage of <strong>puts</strong> in your application code. Output is
intercepted by redefining the global <strong>puts</strong> command while the defined
test script is being run. Errors thrown by C procedures or printed
directly from C applications will not be caught by the <strong>test</strong> command.
Therefore, usage of the <strong>-output</strong> and <strong>-errorOutput</strong> options to
<strong>test</strong> is useful only for pure Tcl applications that use <strong>puts</strong> to
produce output.</p>
<h1 id="keywords">KEYWORDS</h1>
<p>test, test harness, test suite</p>
              
            </div>
          </div><footer>

  <hr/>

  <div role="contentinfo">
    <!-- Copyright etc -->
  </div>

  Built with <a href="https://www.mkdocs.org/">MkDocs</a> using a <a href="https://github.com/readthedocs/sphinx_rtd_theme">theme</a> provided by <a href="https://readthedocs.org">Read the Docs</a>.
</footer>
          
        </div>
      </div>

    </section>

  </div>

  <div class="rst-versions" role="note" aria-label="Versions">
  <span class="rst-current-version" data-toggle="rst-current-version">
    
    
    
  </span>
</div>
    <script>var base_url = '../..';</script>
    <script src="../../js/theme_extra.js" defer></script>
    <script src="../../js/theme.js" defer></script>
      <script src="../../search/main.js" defer></script>
    <script defer>
        window.onload = function () {
            SphinxRtdTheme.Navigation.enable(true);
        };
    </script>

</body>
</html>
